{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from mnist import MNIST\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import markdown as mk\n",
    "from __future__ import division\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement layers as a python function\n",
    "**1. Inner Product Forward Propagation :** <br/>\n",
    ">The input unit is composed of 28 pixel by 28 pixel image in grey scale intensity(between 0 and 1). Here ｗe have 60000 input units.\n",
    "Take each input then multiply them by the weight,sum all of products of all connections to the nodes.<br/>\n",
    "**[60000,748] dot [748,15] ==> [60000,15]**<br/>\n",
    "Here we will add bias b to get the desired function<br/>\n",
    "<br/>\n",
    "\n",
    "**2. Sigmoid Forward Propagation :**<br/>\n",
    ">Sum of the Inner product For_prop is passed through the sigmoid function<br/>\n",
    "<img src=\"figure/sigmoid.png\",width=150,height=150>\n",
    ">It gives the activation of each of the units in the hidden layer to make the output result become nonlinear<br/>\n",
    "The gradient of the sigmoid function vanishes as we increase or decrease x<br/>\n",
    "<img src=\"figure/figure2.png\",width=250,height=250>\n",
    "<br/>\n",
    "\n",
    "**3. Rectified Forward Propagation :**<br/>\n",
    ">It is an another activation function.If sum of the Inner product less than zero then output zero. If sum of the Inner product more than zero then output one\n",
    "<img src=\"figure/relu.png\",width=200,height=200>\n",
    ">the gradient of the ReL function doesn't vanish as we increase x\n",
    "<img src=\"figure/figure1.png\",width=250,height=250>\n",
    "<br/>\n",
    "\n",
    "**4. Softmax Forward Propagation :** <br/>\n",
    ">Softmax function can highlights the largest input and suppresses all the other smaller ones\n",
    "<img src=\"figure/softmax.png\",width=120,height=120>\n",
    ">**for example : softargmax( [3,5,0] ) ≈ [0.12,0.88,0]**\n",
    " \n",
    "**5. Inner Product Backward Propagation :** <br/>\n",
    "Backward Pass calculate the gradient of loss，for updating parameter。This pass goes from top to bottom\n",
    ">Do the partial derivative to get dEdW dEdb dEdx which means the changing value affects the total error\n",
    "\n",
    "**6.  Sigmoid Backward Propagation :**<br/>\n",
    "<img src=\"figure/sigmoid2.png\",width=300,height=300>\n",
    "<img src=\"figure/sigmoid1.png\",width=300,height=300>\n",
    "**7.  Rectified Backward Propagation :**<br/>\n",
    ">if input x > 0 then it will continue propagating, else it will stop\n",
    ">**dEdy[x<=0] = 0**\n",
    "\n",
    "**8.  Softmax Backward Propagation :**<br/>\n",
    ">We need to let our ouput result get closer to the training label, so we can use softmax backward propagation to find out the bias between result and label.<br/>\n",
    ">>**result = [0.3, 0.2 0.5] minus label = [0, 1, 0]** <br/>\n",
    ">>**==> softmax_backprop = [0.3, -0.8, 0.5] ** \n",
    ">if the bias is small ,then it means it get closer to the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def InnerProduct_ForProp(x,W,b):\n",
    "    y = np.dot(x,W)+b\n",
    "    return y\n",
    "\n",
    "\n",
    "def InnerProduct_BackProp(dEdy,x,W,b):\n",
    "\n",
    "    dEdb = dEdy*b    \n",
    "    (n,m) = dEdb.shape\n",
    "    dEdb = np.sum(dEdb) / n\n",
    "    \n",
    "    m = x.shape[1]\n",
    "    dEdW = 1./m * np.dot(x.T,dEdy)\n",
    "    \n",
    "    dEdx = np.dot(dEdy,W.T)\n",
    "    \n",
    "    return dEdx,dEdW,dEdb\n",
    "\n",
    "def Softmax_ForProp(x):\n",
    "    # avoid overflow\n",
    "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    y = exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "    return y\n",
    "\n",
    "\n",
    "def Softmax_BackProp(y,t):\n",
    "    dEdx = y - t;\n",
    "    return dEdx\n",
    "\n",
    "def Sigmoid_ForProp(x):\n",
    "    y = 1 / (1 + np.exp(-x))\n",
    "    return y\n",
    "\n",
    "def Sigmoid_BackProp(dEdy,x):\n",
    "    dEdx = dEdy * x * (1-x)\n",
    "    return dEdx\n",
    "\n",
    "def Rectified_ForProp(x):\n",
    "    y = np.maximum(x,0)\n",
    "    return y\n",
    "\n",
    "def Rectified_BackProp(dEdy,x):\n",
    "    dEdy[x<=0] = 0\n",
    "    dEdx = dEdy\n",
    "    return dEdx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### { Load in the data } \n",
    "The problem we are trying to solve here is to classify grayscale images of handwritten digits (28 pixels by 28 pixels), into their 10 categories (0 to 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000,)\n"
     ]
    }
   ],
   "source": [
    "#load MNIST data\n",
    "mndata = MNIST('./data',return_type='numpy')\n",
    "train_images, train_labels = mndata.load_training()\n",
    "test_images, test_labels = mndata.load_testing()\n",
    "\n",
    "train_labels = np.array(train_labels)\n",
    "print(train_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### { Data Preprocessing }\n",
    "**Image set part:**\n",
    ">Separating the train_image into 7:3. Seven for training and three for validating.<br/>\n",
    ">Beforeｉseparate it, i do **normalization** to make the value of data set in range (0,1) by dividing all of them in **255**\n",
    "\n",
    "** Label set part:**\n",
    ">Because each output result of images will be 10 values set, so we need to do **one hot** to make the label become ten values<br/>\n",
    "**for example: label = 4 => do one hot => [0,0,0,0,1,0,0,0,0,0]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000,)\n",
      "(42000, 10)\n",
      "(18000, 10)\n",
      "(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "#Separate train_image,train_labels into training and validating\n",
    "from __future__ import division\n",
    "\n",
    "\n",
    "def one_hot(x):\n",
    "    #print(x)\n",
    "    ofm = np.zeros(10)\n",
    "    ofm[x] = 1\n",
    "    #print np.array([ofm])\n",
    "    return np.array([ofm])\n",
    "\n",
    "\n",
    "train_images = train_images/255\n",
    "test_images = test_images/255\n",
    " \n",
    "\n",
    "\n",
    "training_image = train_images[:int(len(train_images)*0.7)]\n",
    "validating_image = train_images[int(len(train_images)*0.7):]\n",
    "testing_image = test_images[0:10000]\n",
    "\n",
    "train_labels = np.array(train_labels)\n",
    "test_labels = np.array(test_labels)\n",
    "\n",
    "training_label =  np.empty((0,10),int)\n",
    "validating_label =  np.empty((0,10),int)\n",
    "testing_label =  np.empty((0,10),int)\n",
    "\n",
    "\n",
    "\n",
    "print(train_labels.shape)\n",
    "for i in range(0, int(len(train_images)*0.7)):\n",
    "    training_label = np.append(training_label,one_hot(train_labels[i]),axis=0)\n",
    "for i in range(42000, 60000):\n",
    "    validating_label = np.append(validating_label,one_hot(train_labels[i]),axis=0)\n",
    "for i in range(0, 10000):\n",
    "     testing_label = np.append(testing_label,one_hot(test_labels[i]),axis=0)\n",
    "print (training_label.shape)\n",
    "print (validating_label.shape)\n",
    "print (testing_label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### { Define the parameters }\n",
    "**learning rate : 0.01**<br/>\n",
    "**iteration : 5000**<br/>\n",
    "**decay_rate : 0.8**<br/>\n",
    "**Random the weights & biases at first**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Parameters\n",
    "eta = 0.01       #learning rate\n",
    "size0 = 784     #size of layer 1 (input layer)\n",
    "size1 = 15   #size of layer 2 (1st hidden layer)\n",
    "size2 = 10   #size of layer 3 (output layer)\n",
    "epoch = 25   #epoch for training \n",
    "iteration = 5001;\n",
    "decay_rate = 0.9\n",
    "params = {}\n",
    "params['W1'] = np.random.randn(size0,size1)\n",
    "params['b1'] = np.zeros(size1)\n",
    "params['W2'] = np.random.randn(size1,size2)\n",
    "params['b2'] = np.zeros(size2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Model\n",
    "**1. Forward-propagation : Inner Product - Sigmoid - Inner Product - Softmax** <br/>\n",
    ">*Input* : training image , weight and bias <br/>\n",
    "*Output* : (42000,10) array. Included 42000 image. Each image contains 10 value which represents the confident score about digit number 0-9\n",
    "<img src=\"figure/1-hidden.png\",width=300,height=300>\n",
    "\n",
    "**2. Backward-propagation : Softmax_back - Inner Product_back - Sigmoid_back - Inner Product_back **<br/>\n",
    ">Consider w1. We want to know how much a change in w1 affects the total error<img src=\"figure/dedw.png\",width=300,height=300>\n",
    "\n",
    "**3. Parameter updating**\n",
    ">To decrease the error, we then subtract this value from the current weight (optionally multiplied by some learning rate, eta, which we’ll set to 0.01):\n",
    ">>W1 = W1 - eta * dEW1\n",
    "\n",
    "**4. Do cross validation**\n",
    ">Do the forward propagation again and get the output of \"train_image prediction\" and \"valid_image prediction\"<br/>\n",
    "\n",
    "**5. Get Accuracy**\n",
    ">Comparing the value of them with their label set which we do the one hot before to get the final accuracy\n",
    ">Here i use **zip** function together two lists \"image\" & \"label\" and find their max value's index by using **argmax**<br/>\n",
    "In valid image and valid label,containing 18000 elements each, the result has 18000 elements. Each element is a two-tuple (a,b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round : 0 | Train Accuracy : 10.130952\n",
      "Round : 0 | Valid Accuracy : 9.744444\n",
      "-----------------------------------------\n",
      "Round : 50 | Train Accuracy : 46.333333\n",
      "Round : 50 | Valid Accuracy : 46.366667\n",
      "-----------------------------------------\n",
      "Round : 100 | Train Accuracy : 59.545238\n",
      "Round : 100 | Valid Accuracy : 59.088889\n",
      "-----------------------------------------\n",
      "Round : 150 | Train Accuracy : 59.566667\n",
      "Round : 150 | Valid Accuracy : 60.144444\n",
      "-----------------------------------------\n",
      "Round : 200 | Train Accuracy : 65.050000\n",
      "Round : 200 | Valid Accuracy : 65.794444\n",
      "-----------------------------------------\n",
      "Round : 250 | Train Accuracy : 69.095238\n",
      "Round : 250 | Valid Accuracy : 69.811111\n",
      "-----------------------------------------\n",
      "Round : 300 | Train Accuracy : 72.530952\n",
      "Round : 300 | Valid Accuracy : 73.194444\n",
      "-----------------------------------------\n",
      "Round : 350 | Train Accuracy : 75.319048\n",
      "Round : 350 | Valid Accuracy : 75.822222\n",
      "-----------------------------------------\n",
      "Round : 400 | Train Accuracy : 77.259524\n",
      "Round : 400 | Valid Accuracy : 77.772222\n",
      "-----------------------------------------\n",
      "Round : 450 | Train Accuracy : 78.690476\n",
      "Round : 450 | Valid Accuracy : 79.261111\n",
      "-----------------------------------------\n",
      "Round : 500 | Train Accuracy : 79.971429\n",
      "Round : 500 | Valid Accuracy : 80.283333\n",
      "-----------------------------------------\n",
      "Round : 550 | Train Accuracy : 80.971429\n",
      "Round : 550 | Valid Accuracy : 81.255556\n",
      "-----------------------------------------\n",
      "Round : 600 | Train Accuracy : 81.883333\n",
      "Round : 600 | Valid Accuracy : 82.055556\n",
      "-----------------------------------------\n",
      "Round : 650 | Train Accuracy : 82.595238\n",
      "Round : 650 | Valid Accuracy : 82.733333\n",
      "-----------------------------------------\n",
      "Round : 700 | Train Accuracy : 83.214286\n",
      "Round : 700 | Valid Accuracy : 83.200000\n",
      "-----------------------------------------\n",
      "Round : 750 | Train Accuracy : 83.850000\n",
      "Round : 750 | Valid Accuracy : 83.800000\n",
      "-----------------------------------------\n",
      "Round : 800 | Train Accuracy : 84.342857\n",
      "Round : 800 | Valid Accuracy : 84.172222\n",
      "-----------------------------------------\n",
      "Round : 850 | Train Accuracy : 84.819048\n",
      "Round : 850 | Valid Accuracy : 84.627778\n",
      "-----------------------------------------\n",
      "Round : 900 | Train Accuracy : 85.216667\n",
      "Round : 900 | Valid Accuracy : 84.938889\n",
      "-----------------------------------------\n",
      "Round : 950 | Train Accuracy : 85.547619\n",
      "Round : 950 | Valid Accuracy : 85.216667\n",
      "-----------------------------------------\n",
      "Round : 1000 | Train Accuracy : 85.850000\n",
      "Round : 1000 | Valid Accuracy : 85.672222\n",
      "-----------------------------------------\n",
      "decay!!...........\n",
      "Round : 1050 | Train Accuracy : 86.519048\n",
      "Round : 1050 | Valid Accuracy : 86.183333\n",
      "-----------------------------------------\n",
      "Round : 1100 | Train Accuracy : 86.778571\n",
      "Round : 1100 | Valid Accuracy : 86.388889\n",
      "-----------------------------------------\n",
      "Round : 1150 | Train Accuracy : 86.978571\n",
      "Round : 1150 | Valid Accuracy : 86.638889\n",
      "-----------------------------------------\n",
      "Round : 1200 | Train Accuracy : 87.080952\n",
      "Round : 1200 | Valid Accuracy : 86.844444\n",
      "-----------------------------------------\n",
      "Round : 1250 | Train Accuracy : 87.211905\n",
      "Round : 1250 | Valid Accuracy : 86.972222\n",
      "-----------------------------------------\n",
      "Round : 1300 | Train Accuracy : 87.423810\n",
      "Round : 1300 | Valid Accuracy : 87.177778\n",
      "-----------------------------------------\n",
      "Round : 1350 | Train Accuracy : 87.602381\n",
      "Round : 1350 | Valid Accuracy : 87.322222\n",
      "-----------------------------------------\n",
      "Round : 1400 | Train Accuracy : 87.752381\n",
      "Round : 1400 | Valid Accuracy : 87.494444\n",
      "-----------------------------------------\n",
      "Round : 1450 | Train Accuracy : 87.909524\n",
      "Round : 1450 | Valid Accuracy : 87.605556\n",
      "-----------------------------------------\n",
      "Round : 1500 | Train Accuracy : 88.083333\n",
      "Round : 1500 | Valid Accuracy : 87.750000\n",
      "-----------------------------------------\n",
      "Round : 1550 | Train Accuracy : 88.223810\n",
      "Round : 1550 | Valid Accuracy : 87.888889\n",
      "-----------------------------------------\n",
      "Round : 1600 | Train Accuracy : 88.366667\n",
      "Round : 1600 | Valid Accuracy : 87.961111\n",
      "-----------------------------------------\n",
      "Round : 1650 | Train Accuracy : 88.488095\n",
      "Round : 1650 | Valid Accuracy : 88.088889\n",
      "-----------------------------------------\n",
      "Round : 1700 | Train Accuracy : 88.635714\n",
      "Round : 1700 | Valid Accuracy : 88.194444\n",
      "-----------------------------------------\n",
      "Round : 1750 | Train Accuracy : 88.785714\n",
      "Round : 1750 | Valid Accuracy : 88.327778\n",
      "-----------------------------------------\n",
      "Round : 1800 | Train Accuracy : 88.911905\n",
      "Round : 1800 | Valid Accuracy : 88.433333\n",
      "-----------------------------------------\n",
      "Round : 1850 | Train Accuracy : 89.019048\n",
      "Round : 1850 | Valid Accuracy : 88.500000\n",
      "-----------------------------------------\n",
      "Round : 1900 | Train Accuracy : 89.133333\n",
      "Round : 1900 | Valid Accuracy : 88.577778\n",
      "-----------------------------------------\n",
      "Round : 1950 | Train Accuracy : 89.190476\n",
      "Round : 1950 | Valid Accuracy : 88.694444\n",
      "-----------------------------------------\n",
      "Round : 2000 | Train Accuracy : 89.266667\n",
      "Round : 2000 | Valid Accuracy : 88.772222\n",
      "-----------------------------------------\n",
      "decay!!...........\n",
      "Round : 2050 | Train Accuracy : 89.350000\n",
      "Round : 2050 | Valid Accuracy : 88.850000\n",
      "-----------------------------------------\n",
      "Round : 2100 | Train Accuracy : 89.402381\n",
      "Round : 2100 | Valid Accuracy : 88.883333\n",
      "-----------------------------------------\n",
      "Round : 2150 | Train Accuracy : 89.464286\n",
      "Round : 2150 | Valid Accuracy : 88.983333\n",
      "-----------------------------------------\n",
      "Round : 2200 | Train Accuracy : 89.511905\n",
      "Round : 2200 | Valid Accuracy : 88.994444\n",
      "-----------------------------------------\n",
      "Round : 2250 | Train Accuracy : 89.552381\n",
      "Round : 2250 | Valid Accuracy : 89.088889\n",
      "-----------------------------------------\n",
      "Round : 2300 | Train Accuracy : 89.614286\n",
      "Round : 2300 | Valid Accuracy : 89.105556\n",
      "-----------------------------------------\n",
      "Round : 2350 | Train Accuracy : 89.704762\n",
      "Round : 2350 | Valid Accuracy : 89.166667\n",
      "-----------------------------------------\n",
      "Round : 2400 | Train Accuracy : 89.773810\n",
      "Round : 2400 | Valid Accuracy : 89.238889\n",
      "-----------------------------------------\n",
      "Round : 2450 | Train Accuracy : 89.864286\n",
      "Round : 2450 | Valid Accuracy : 89.288889\n",
      "-----------------------------------------\n",
      "Round : 2500 | Train Accuracy : 89.895238\n",
      "Round : 2500 | Valid Accuracy : 89.338889\n",
      "-----------------------------------------\n",
      "Round : 2550 | Train Accuracy : 89.926190\n",
      "Round : 2550 | Valid Accuracy : 89.366667\n",
      "-----------------------------------------\n",
      "Round : 2600 | Train Accuracy : 89.971429\n",
      "Round : 2600 | Valid Accuracy : 89.377778\n",
      "-----------------------------------------\n",
      "Round : 2650 | Train Accuracy : 90.021429\n",
      "Round : 2650 | Valid Accuracy : 89.416667\n",
      "-----------------------------------------\n",
      "Round : 2700 | Train Accuracy : 90.092857\n",
      "Round : 2700 | Valid Accuracy : 89.500000\n",
      "-----------------------------------------\n",
      "Round : 2750 | Train Accuracy : 90.138095\n",
      "Round : 2750 | Valid Accuracy : 89.533333\n",
      "-----------------------------------------\n",
      "Round : 2800 | Train Accuracy : 90.183333\n",
      "Round : 2800 | Valid Accuracy : 89.561111\n",
      "-----------------------------------------\n",
      "Round : 2850 | Train Accuracy : 90.257143\n",
      "Round : 2850 | Valid Accuracy : 89.594444\n",
      "-----------------------------------------\n",
      "Round : 2900 | Train Accuracy : 90.314286\n",
      "Round : 2900 | Valid Accuracy : 89.644444\n",
      "-----------------------------------------\n",
      "Round : 2950 | Train Accuracy : 90.373810\n",
      "Round : 2950 | Valid Accuracy : 89.705556\n",
      "-----------------------------------------\n",
      "Round : 3000 | Train Accuracy : 90.426190\n",
      "Round : 3000 | Valid Accuracy : 89.716667\n",
      "-----------------------------------------\n",
      "decay!!...........\n",
      "Round : 3050 | Train Accuracy : 90.447619\n",
      "Round : 3050 | Valid Accuracy : 89.738889\n",
      "-----------------------------------------\n",
      "Round : 3100 | Train Accuracy : 90.473810\n",
      "Round : 3100 | Valid Accuracy : 89.783333\n",
      "-----------------------------------------\n",
      "Round : 3150 | Train Accuracy : 90.488095\n",
      "Round : 3150 | Valid Accuracy : 89.794444\n",
      "-----------------------------------------\n",
      "Round : 3200 | Train Accuracy : 90.530952\n",
      "Round : 3200 | Valid Accuracy : 89.827778\n",
      "-----------------------------------------\n",
      "Round : 3250 | Train Accuracy : 90.542857\n",
      "Round : 3250 | Valid Accuracy : 89.844444\n",
      "-----------------------------------------\n",
      "Round : 3300 | Train Accuracy : 90.566667\n",
      "Round : 3300 | Valid Accuracy : 89.883333\n",
      "-----------------------------------------\n",
      "Round : 3350 | Train Accuracy : 90.602381\n",
      "Round : 3350 | Valid Accuracy : 89.866667\n",
      "-----------------------------------------\n",
      "Round : 3400 | Train Accuracy : 90.633333\n",
      "Round : 3400 | Valid Accuracy : 89.883333\n",
      "-----------------------------------------\n",
      "Round : 3450 | Train Accuracy : 90.661905\n",
      "Round : 3450 | Valid Accuracy : 89.894444\n",
      "-----------------------------------------\n",
      "Round : 3500 | Train Accuracy : 90.700000\n",
      "Round : 3500 | Valid Accuracy : 89.916667\n",
      "-----------------------------------------\n",
      "Round : 3550 | Train Accuracy : 90.733333\n",
      "Round : 3550 | Valid Accuracy : 89.944444\n",
      "-----------------------------------------\n",
      "Round : 3600 | Train Accuracy : 90.766667\n",
      "Round : 3600 | Valid Accuracy : 89.950000\n",
      "-----------------------------------------\n",
      "Round : 3650 | Train Accuracy : 90.823810\n",
      "Round : 3650 | Valid Accuracy : 89.988889\n",
      "-----------------------------------------\n",
      "Round : 3700 | Train Accuracy : 90.840476\n",
      "Round : 3700 | Valid Accuracy : 90.011111\n",
      "-----------------------------------------\n",
      "Round : 3750 | Train Accuracy : 90.864286\n",
      "Round : 3750 | Valid Accuracy : 90.005556\n",
      "-----------------------------------------\n",
      "Round : 3800 | Train Accuracy : 90.888095\n",
      "Round : 3800 | Valid Accuracy : 90.027778\n",
      "-----------------------------------------\n",
      "Round : 3850 | Train Accuracy : 90.921429\n",
      "Round : 3850 | Valid Accuracy : 90.044444\n",
      "-----------------------------------------\n",
      "Round : 3900 | Train Accuracy : 90.938095\n",
      "Round : 3900 | Valid Accuracy : 90.061111\n",
      "-----------------------------------------\n",
      "Round : 3950 | Train Accuracy : 90.973810\n",
      "Round : 3950 | Valid Accuracy : 90.094444\n",
      "-----------------------------------------\n",
      "Round : 4000 | Train Accuracy : 91.000000\n",
      "Round : 4000 | Valid Accuracy : 90.133333\n",
      "-----------------------------------------\n",
      "decay!!...........\n",
      "Round : 4050 | Train Accuracy : 91.014286\n",
      "Round : 4050 | Valid Accuracy : 90.111111\n",
      "-----------------------------------------\n",
      "Round : 4100 | Train Accuracy : 91.028571\n",
      "Round : 4100 | Valid Accuracy : 90.105556\n",
      "-----------------------------------------\n",
      "Round : 4150 | Train Accuracy : 91.057143\n",
      "Round : 4150 | Valid Accuracy : 90.138889\n",
      "-----------------------------------------\n",
      "Round : 4200 | Train Accuracy : 91.073810\n",
      "Round : 4200 | Valid Accuracy : 90.144444\n",
      "-----------------------------------------\n",
      "Round : 4250 | Train Accuracy : 91.083333\n",
      "Round : 4250 | Valid Accuracy : 90.155556\n",
      "-----------------------------------------\n",
      "Round : 4300 | Train Accuracy : 91.097619\n",
      "Round : 4300 | Valid Accuracy : 90.144444\n",
      "-----------------------------------------\n",
      "Round : 4350 | Train Accuracy : 91.111905\n",
      "Round : 4350 | Valid Accuracy : 90.144444\n",
      "-----------------------------------------\n",
      "Round : 4400 | Train Accuracy : 91.133333\n",
      "Round : 4400 | Valid Accuracy : 90.161111\n",
      "-----------------------------------------\n",
      "Round : 4450 | Train Accuracy : 91.159524\n",
      "Round : 4450 | Valid Accuracy : 90.172222\n",
      "-----------------------------------------\n",
      "Round : 4500 | Train Accuracy : 91.185714\n",
      "Round : 4500 | Valid Accuracy : 90.194444\n",
      "-----------------------------------------\n",
      "Round : 4550 | Train Accuracy : 91.207143\n",
      "Round : 4550 | Valid Accuracy : 90.205556\n",
      "-----------------------------------------\n",
      "Round : 4600 | Train Accuracy : 91.228571\n",
      "Round : 4600 | Valid Accuracy : 90.222222\n",
      "-----------------------------------------\n",
      "Round : 4650 | Train Accuracy : 91.254762\n",
      "Round : 4650 | Valid Accuracy : 90.227778\n",
      "-----------------------------------------\n",
      "Round : 4700 | Train Accuracy : 91.278571\n",
      "Round : 4700 | Valid Accuracy : 90.222222\n",
      "-----------------------------------------\n",
      "Round : 4750 | Train Accuracy : 91.283333\n",
      "Round : 4750 | Valid Accuracy : 90.266667\n",
      "-----------------------------------------\n",
      "Round : 4800 | Train Accuracy : 91.302381\n",
      "Round : 4800 | Valid Accuracy : 90.294444\n",
      "-----------------------------------------\n",
      "Round : 4850 | Train Accuracy : 91.314286\n",
      "Round : 4850 | Valid Accuracy : 90.300000\n",
      "-----------------------------------------\n",
      "Round : 4900 | Train Accuracy : 91.333333\n",
      "Round : 4900 | Valid Accuracy : 90.316667\n",
      "-----------------------------------------\n",
      "Round : 4950 | Train Accuracy : 91.357143\n",
      "Round : 4950 | Valid Accuracy : 90.338889\n",
      "-----------------------------------------\n",
      "Round : 5000 | Train Accuracy : 91.366667\n",
      "Round : 5000 | Valid Accuracy : 90.355556\n",
      "-----------------------------------------\n",
      "decay!!...........\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "\n",
    "W1, b1 = params['W1'], params['b1']\n",
    "W2, b2 = params['W2'], params['b2']\n",
    "\n",
    "validAccArray = np.empty((0,18000),float)\n",
    "trainAccArray = np.empty((0,42000),float)\n",
    "\n",
    "for iternum in range(iteration):\n",
    "    vcorrect = []\n",
    "    tcorrect = []\n",
    "    \n",
    "    # Forward-propagation\n",
    "    a1 = InnerProduct_ForProp(training_image,W1,b1)\n",
    "    z1 = Sigmoid_ForProp(a1)\n",
    "    a2 = InnerProduct_ForProp(z1,W2,b2)\n",
    "    out = Softmax_ForProp(a2)\n",
    "    \n",
    "    # Bakcward-propagation\n",
    "    dEda2 =  Softmax_BackProp(out,training_label)\n",
    "    dEz1,dEW2,dEb2 = InnerProduct_BackProp(dEda2,z1,W2,b2)\n",
    "    dEa1 = Sigmoid_BackProp(dEz1,z1)\n",
    "    dEdz0,dEW1,dEb1 = InnerProduct_BackProp(dEa1,training_image,W1,b1)\n",
    "    \n",
    "    # Parameters Updating (Gradient descent)\n",
    "    \n",
    "    W1 = W1 - eta*dEW1\n",
    "    W2 = W2 - eta*dEW2\n",
    "    b1 = b1 - eta*dEb1\n",
    "    b2 = b2 - eta*dEb2\n",
    "    \n",
    "    # Do cross-validation to evaluate model\n",
    "    \n",
    "    z3 = InnerProduct_ForProp(training_image,W1,b1)\n",
    "    a3 = Sigmoid_ForProp(z3)\n",
    "    z4 = InnerProduct_ForProp(a3,W2,b2)\n",
    "    trainPrediction = Softmax_ForProp(z4)\n",
    "    \n",
    "    z5 = InnerProduct_ForProp(validating_image,W1,b1)\n",
    "    a4 = Sigmoid_ForProp(z5)\n",
    "    z6 = InnerProduct_ForProp(a4,W2,b2)\n",
    "    validPrediction = Softmax_ForProp(z6)\n",
    "    \n",
    "   \n",
    "    tcorrect = [1 if a==b else 0 for (a,b) in zip(np.argmax(trainPrediction,axis=1),np.argmax(training_label,axis=1))]\n",
    "    #print np.sum(tcorrect)       \n",
    "    trainAccArray = np.append(trainAccArray, float(np.sum(tcorrect))/42000*100)\n",
    "    #print tAccuracy\n",
    "    #print  \"Round : %d | Train Accuracy : %f\"%(iternum, trainAccArray[iternum])\n",
    "    \n",
    "    vcorrect = [1 if a==b else 0 for (a,b) in zip(np.argmax(validPrediction,axis=1),np.argmax(validating_label,axis=1))]\n",
    "    #print np.sum(vcorrect)   \n",
    "    validAccArray = np.append(validAccArray, float(np.sum(vcorrect))/18000*100)\n",
    "    #print  \"Round : %d | Valid Accuracy : %f\"%(iternum, validAccArray[iternum])\n",
    "    #print\"---------------------------------------\"\n",
    "    if iternum%50 ==0:\n",
    "        print  \"Round : %d | Train Accuracy : %f\"%(iternum, trainAccArray[iternum])\n",
    "        print  \"Round : %d | Valid Accuracy : %f\"%(iternum, validAccArray[iternum])\n",
    "        print\"-----------------------------------------\"\n",
    "    if  iternum!=0 and iternum%1000==0:\n",
    "        print \"decay!!...........\"\n",
    "        eta = eta * (decay_rate**(iternum/1000))\n",
    "        #print eta  \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using test_images and test_labels to do the final test\n",
    "\n",
    "**1. Do cross validation**\n",
    ">Do the forward propagation again and get the output of \"test_image prediction\"\"<br/>\n",
    "\n",
    "**2. Get Accuracy**\n",
    ">Comparing the value of them with their label set which we do the one hot before to get the final accuracy\n",
    ">Here i use **zip** function together two lists \"image\" & \"label\" and find their max value's index by using **argmax**<br/>\n",
    "In test image and test label,containing 10000 elements each, the result has 10000 elements. Each element is a two-tuple (a,b)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round : 0 | Test Accuracy : 12.720000\n",
      "Round : 50 | Test Accuracy : 53.700000\n",
      "Round : 100 | Test Accuracy : 59.480000\n",
      "Round : 150 | Test Accuracy : 64.310000\n",
      "Round : 200 | Test Accuracy : 68.100000\n",
      "Round : 250 | Test Accuracy : 70.910000\n",
      "Round : 300 | Test Accuracy : 73.090000\n",
      "Round : 350 | Test Accuracy : 75.150000\n",
      "Round : 400 | Test Accuracy : 76.510000\n",
      "Round : 450 | Test Accuracy : 77.680000\n",
      "Round : 500 | Test Accuracy : 78.740000\n",
      "Round : 550 | Test Accuracy : 79.380000\n",
      "Round : 600 | Test Accuracy : 80.040000\n",
      "Round : 650 | Test Accuracy : 80.730000\n",
      "Round : 700 | Test Accuracy : 81.390000\n",
      "Round : 750 | Test Accuracy : 81.970000\n",
      "Round : 800 | Test Accuracy : 82.480000\n",
      "Round : 850 | Test Accuracy : 82.930000\n",
      "Round : 900 | Test Accuracy : 83.500000\n",
      "Round : 950 | Test Accuracy : 83.990000\n",
      "Round : 1000 | Test Accuracy : 84.250000\n",
      "decay!!...........\n",
      "Round : 1050 | Test Accuracy : 85.600000\n",
      "Round : 1100 | Test Accuracy : 85.540000\n",
      "Round : 1150 | Test Accuracy : 85.800000\n",
      "Round : 1200 | Test Accuracy : 86.010000\n",
      "Round : 1250 | Test Accuracy : 86.210000\n",
      "Round : 1300 | Test Accuracy : 86.420000\n",
      "Round : 1350 | Test Accuracy : 86.540000\n",
      "Round : 1400 | Test Accuracy : 86.650000\n",
      "Round : 1450 | Test Accuracy : 86.810000\n",
      "Round : 1500 | Test Accuracy : 86.940000\n",
      "Round : 1550 | Test Accuracy : 87.130000\n",
      "Round : 1600 | Test Accuracy : 87.200000\n",
      "Round : 1650 | Test Accuracy : 87.320000\n",
      "Round : 1700 | Test Accuracy : 87.420000\n",
      "Round : 1750 | Test Accuracy : 87.480000\n",
      "Round : 1800 | Test Accuracy : 87.520000\n",
      "Round : 1850 | Test Accuracy : 87.660000\n",
      "Round : 1900 | Test Accuracy : 87.740000\n",
      "Round : 1950 | Test Accuracy : 87.870000\n",
      "Round : 2000 | Test Accuracy : 87.930000\n",
      "decay!!...........\n",
      "Round : 2050 | Test Accuracy : 87.980000\n",
      "Round : 2100 | Test Accuracy : 88.070000\n",
      "Round : 2150 | Test Accuracy : 88.130000\n",
      "Round : 2200 | Test Accuracy : 88.220000\n",
      "Round : 2250 | Test Accuracy : 88.330000\n",
      "Round : 2300 | Test Accuracy : 88.370000\n",
      "Round : 2350 | Test Accuracy : 88.380000\n",
      "Round : 2400 | Test Accuracy : 88.430000\n",
      "Round : 2450 | Test Accuracy : 88.470000\n",
      "Round : 2500 | Test Accuracy : 88.500000\n",
      "Round : 2550 | Test Accuracy : 88.520000\n",
      "Round : 2600 | Test Accuracy : 88.600000\n",
      "Round : 2650 | Test Accuracy : 88.670000\n",
      "Round : 2700 | Test Accuracy : 88.760000\n",
      "Round : 2750 | Test Accuracy : 88.760000\n",
      "Round : 2800 | Test Accuracy : 88.850000\n",
      "Round : 2850 | Test Accuracy : 88.850000\n",
      "Round : 2900 | Test Accuracy : 88.860000\n",
      "Round : 2950 | Test Accuracy : 88.900000\n",
      "Round : 3000 | Test Accuracy : 88.950000\n",
      "decay!!...........\n",
      "Round : 3050 | Test Accuracy : 88.980000\n",
      "Round : 3100 | Test Accuracy : 88.990000\n",
      "Round : 3150 | Test Accuracy : 89.000000\n",
      "Round : 3200 | Test Accuracy : 89.030000\n",
      "Round : 3250 | Test Accuracy : 89.060000\n",
      "Round : 3300 | Test Accuracy : 89.110000\n",
      "Round : 3350 | Test Accuracy : 89.130000\n",
      "Round : 3400 | Test Accuracy : 89.190000\n",
      "Round : 3450 | Test Accuracy : 89.230000\n",
      "Round : 3500 | Test Accuracy : 89.260000\n",
      "Round : 3550 | Test Accuracy : 89.300000\n",
      "Round : 3600 | Test Accuracy : 89.290000\n",
      "Round : 3650 | Test Accuracy : 89.350000\n",
      "Round : 3700 | Test Accuracy : 89.400000\n",
      "Round : 3750 | Test Accuracy : 89.440000\n",
      "Round : 3800 | Test Accuracy : 89.460000\n",
      "Round : 3850 | Test Accuracy : 89.480000\n",
      "Round : 3900 | Test Accuracy : 89.500000\n",
      "Round : 3950 | Test Accuracy : 89.510000\n",
      "Round : 4000 | Test Accuracy : 89.610000\n",
      "decay!!...........\n",
      "Round : 4050 | Test Accuracy : 89.650000\n",
      "Round : 4100 | Test Accuracy : 89.700000\n",
      "Round : 4150 | Test Accuracy : 89.710000\n",
      "Round : 4200 | Test Accuracy : 89.680000\n",
      "Round : 4250 | Test Accuracy : 89.720000\n",
      "Round : 4300 | Test Accuracy : 89.750000\n",
      "Round : 4350 | Test Accuracy : 89.790000\n",
      "Round : 4400 | Test Accuracy : 89.840000\n",
      "Round : 4450 | Test Accuracy : 89.870000\n",
      "Round : 4500 | Test Accuracy : 89.870000\n",
      "Round : 4550 | Test Accuracy : 89.880000\n",
      "Round : 4600 | Test Accuracy : 89.910000\n",
      "Round : 4650 | Test Accuracy : 89.900000\n",
      "Round : 4700 | Test Accuracy : 89.890000\n",
      "Round : 4750 | Test Accuracy : 89.900000\n",
      "Round : 4800 | Test Accuracy : 89.900000\n",
      "Round : 4850 | Test Accuracy : 89.930000\n",
      "Round : 4900 | Test Accuracy : 89.960000\n",
      "Round : 4950 | Test Accuracy : 89.980000\n",
      "Round : 5000 | Test Accuracy : 90.000000\n",
      "decay!!...........\n"
     ]
    }
   ],
   "source": [
    "#Using test_images and test_labels to do the final test\n",
    "from __future__ import division\n",
    "\n",
    "W1, b1 = params['W1'], params['b1']\n",
    "W2, b2 = params['W2'], params['b2']\n",
    "\n",
    "testAccArray = np.empty((0,10000),float)\n",
    "\n",
    "\n",
    "for iternum in range(iteration):\n",
    "    tcorrect = []\n",
    "    # Forward-propagation\n",
    "    a1 = InnerProduct_ForProp(training_image,W1,b1)\n",
    "    z1 = Sigmoid_ForProp(a1)\n",
    "    a2 = InnerProduct_ForProp(z1,W2,b2)\n",
    "    out = Softmax_ForProp(a2)\n",
    "    \n",
    "    # Bakcward-propagation\n",
    "    dEda2 =  Softmax_BackProp(out,training_label)\n",
    "    dEz1,dEW2,dEb2 = InnerProduct_BackProp(dEda2,z1,W2,b2)\n",
    "    dEa1 = Sigmoid_BackProp(dEz1,z1)\n",
    "    dEdz0,dEW1,dEb1 = InnerProduct_BackProp(dEa1,training_image,W1,b1)\n",
    "    \n",
    "    # Parameters Updating (Gradient descent)\n",
    "    \n",
    "    W1 = W1 - eta*dEW1\n",
    "    W2 = W2 - eta*dEW2\n",
    "    b1 = b1 - eta*dEb1\n",
    "    b2 = b2 - eta*dEb2\n",
    "    \n",
    "    # Do cross-validation to evaluate model\n",
    "    \n",
    "    z3 = InnerProduct_ForProp(testing_image,W1,b1)\n",
    "    a3 = Sigmoid_ForProp(z3)\n",
    "    z4 = InnerProduct_ForProp(a3,W2,b2)\n",
    "    trainPrediction = Softmax_ForProp(z4)\n",
    "\n",
    "    \n",
    "    #print (Prediction[0])\n",
    "    #print np.max(Prediction[0])\n",
    "    #print np.argmax((Prediction[0]))\n",
    "    #print (validating_label[0])\n",
    "    #print np.argmax(validating_label[0])\n",
    "    \n",
    "   \n",
    "    tcorrect = [1 if a==b else 0 for (a,b) in zip(np.argmax(trainPrediction,axis=1),np.argmax(testing_label,axis=1))]    \n",
    "    testAccArray = np.append(testAccArray, float(np.sum(tcorrect))/10000*100)\n",
    "    #print tAccuracy\n",
    "    #print  \"Round : %d | Test Accuracy : %f\"%(iternum, testAccArray[iternum])\n",
    "      \n",
    "    if iternum%50 ==0:\n",
    "        print  \"Round : %d | Test Accuracy : %f\"%(iternum, testAccArray[iternum])\n",
    "    \n",
    "    if  iternum!=0 and iternum%1000==0:\n",
    "        print \"decay!!...........\"\n",
    "        eta = eta * (decay_rate**(iternum/1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot iter accuracy curve and store accuracy in txt\n",
    "** Print out accuracy of test, train, valid set **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.savetxt(\"../project1/accuracy_txt/trainacc.txt\", trainAccArray , fmt=\"%f\")\n",
    "np.savetxt(\"../project1/accuracy_txt/validacc.txt\", validAccArray , fmt=\"%f\")\n",
    "np.savetxt(\"../project1/accuracy_txt/testacc.txt\", testAccArray , fmt=\"%f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------\n",
      "Max Train ACC : 91.366667\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAEACAYAAAC6d6FnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSP1/rlB5lsOEoAgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1080508d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------\n",
      "Max Valid ACC : 90.355556\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAEACAYAAAC6d6FnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAHx9JREFUeJzt3XmcFNW99/HPr3tmgGGcAQYGcHAQJW6PRh9uIBFNGCMi\nuF4fxEcIglvMK8kVick1aGIg5iYu0bjcJOb6iiYIiKA+uCUEVO4QlytxQRw3VJQ9DLggjiyz9Hn+\n6OqmmYUZppfq7vq+tV9Un6o69asDfX5dp5Y25xwiIhI8Ib8DEBERfygBiIgElBKAiEhAKQGIiASU\nEoCISEApAYiIBFSHCcDM7jWzOjN7PaGst5ktNbPVZrbEzMoS5t1lZu+Z2WtmdkK6AhcRkeR05gjg\nT8DpLcpmAE87544ElgHXApjZOOBw59yXgO8Af0hhrCIikkIdJgDn3HPApy2KzwVme9Ozvfex8vu9\n9VYAZWbWPzWhiohIKnX1HECFc64OwDm3BajwyiuBDQnLbfLKREQky6T6JLC1UaZnTYiIZKGCLq5X\nZ2b9nXN1ZjYA2OqVbwQOSVhuELC5rQrMTIlBRKQLnHNtfdk+YJ09AjD2/Xb/OHCxN30x8FhC+RQA\nM/sasD02VNQW55xezjFz5kzfY8iWl9pCbaG22P8rlTo8AjCzB4BqoNzM1gMzgZuAh8zsUmA9MMHr\n0P9qZmeY2fvAF8AlKY1WRERSpsME4Jyb1M6s0e0s/29JRSQiIhmhO4GzQHV1td8hZA21xV5qi73U\nFulhqR5T6vSGzZxf2xYRyVVmhsvwSWAREckzSgAiIgGlBCAiElBKACIiAaUEICISUEoAIiIBpQQg\nIhJQSgAiIgGlBCAiElBKACIiAaUEICISUEoAIiIBpQQgIhJQXf1JSBGRQHHOEXERIkSiv85F9H2r\ncueIEInPa3bNNEYaaYo07ftyTTQ2N0aXjUT2WSexvth0JBKhyTWldJ+UAESyXKwTaXbNNEWa2NW0\nK96JRFy0U4hEIjS75mgHgmvV2exq3kVzpLlVJ9WyI4tNR1wEgIiLxLcbqz++Xe/PWAzxzsqLIfZn\nW+VNkSZ2N++O1x+JRPZOJ9ZNOx1iQl2JHW3iNh0u3n6x/yIuAo74+zaX89qmveXyiRKACNEPf5Nr\nYk/THnY376ahuSHeocQ6qFhHWN9YT0NzAw2RBhqbG6lvrGdP8x4amxtpjDTSEGlgZ+POeOcbW68x\n0khDcwONkehye5r3sLNx5z7lifObI83xjl8kHZQAJOvEvm01R5rjnWFDcwOf7P6Ebbu20djcGO1U\nXVO8k9y+ezv1jfXR5RM64sbmRppcE7sad7Grad/X7ubd7Gnew+6maIefj9/wJLVCFiJECAxChAhZ\nCDOL/onFp0NEy43o+6JwEQWhAgqsIPqn9yoMFRK28N71Yi9C+5TF6ikIFfAGb6Rsf/SLYJJyERdh\n686tbPliC0+seYKF7y70O6ScF7Zw9BUK06OgR7wzCYfC8XmhUIiwhQFadTTdwt0oChcBLTqxNjqr\nkIX2Lud1PoXhQgqsIF5WEIpOhy28z/t9OsJ2OrFYWY9wD8Kh8D71JNYXm07sYON1efUnzk98H4sB\niE/H1gf2LpMwL7E8tr3o/9H/wqFwpv/a25TKXwTTEYC0K5agm1wTS9cuZcazM3yOKL3CFqZbuBvd\nC7pTGCps1cnFOqaSwhKKwkUUhYooDBfSs7An3cPdKQoXURgqpChcRI+CHvE6wqFwvEOO1R17FRcW\nUxgqpFu4G4XhwmidXh2J2491ZiKppAQQMBEX4dW6V5nz1hyWbVjmdzj7FTtEjnWkZd3K6F/cn+7h\n7vFvjrFvwT0Le9KrW699OubYuuFQmOKCYnoU9Njn1b2ge6sOXyRI9C8+TzjneLnuZS5dcqnfoSRl\ndNVobv7GzfHhChFJHyWAHLGneQ/31t7Lv/T/Fy5fernf4XTJNwZ9g5+P/Dnl3cs1pCGSBZQAsohz\njm27tnHpkktZt2Od3+G08rtTf8dJB5+UNSfDRCQ5SgA+e2nLS1zx1BVUHVTFJ7s/Yfue7Rnd/o++\n8iOGDxjOl3p/icJQYUa3LSL+0mWgPnDOccVTV/DiP19M2zZuHXUr36z6JgVWoOEWkTyiy0BziHOO\nW1++lfvfuj/ldddOrU15nSISHEoAKeac48v3f7lL6y4Zv4QBPQfEb1bZ8sUWTnv4tPi8g0sOTlmc\nIiIaAkqBe16/h/9c+Z9J1bF0/FIGlgxMUUQikq80BJQFjpt9XErqmXXiLMYfMT4ldYmIHAglgAPw\n0a6POGXhKUnV8ePhP2byMZNTFJGISNcpAXSgdlstk/46qcvrf/u4bzNt2LQURiQikhpKAO34/jPf\n5+8b/96ldV+f8rouvRSRrKcE0MJvXvkNf3rjTwe0TkGogFcmvxK/ekdEJBcoAXj+sOoP/O6133V6\n+Tnj5nBCxQlpjEhEJL2SSgBm9gPgMiAC1AKXAAcDDwK9gVeBi5xL8S8Zp9Dm+s2c/sjpnVp2dNVo\nbj/l9jRHJCKSGV2+D8DMDgaeA45yzjWY2QLgr8AZwMPOuYfM7G7gNefcf7Wxvu/3Afyw5ocsXbe0\nw+UW/5/FDDpoUAYiEhHZv2y6DyAM9DSzCNAD2AycAkz05s8GZgGtEoCfPm/4nJHzR3a43PMTn6e0\nqDQDEYmIZF6Xz1o65zYDtwHrgU3AZ0SHfLY75yLeYhuJDglljeufv77Dzv+nX/0ptVNr1fmLSF7r\n8hGAmfUCzgUGE+38HwLGtbFou+M8s2bNik9XV1dTXV3d1XA6pTN37+oBayKSTWpqaqipqUlL3cmc\nAzgfON05923v/UXAicD5wADnXMTMvgbMdM61SgyZPAfw2Z7POPnBk/e7zIKzFnBM+TEZiUdEpKuy\n5RzAeuBrZtYd2AOcCrwElAMTgAXAVOCxZINMRmNz4347f8N4ferrGYxIRCQ7JPU0UDObCVwINAIr\ngcuBQey9DHQlMNk519jGumk/Aoi4CMfff3y78x8++2GO7HNkWmMQEUmlVB4B5O3joDt6Lv+qKat0\n566I5JxUJoC87QHV+YuI7F9ePgri9Ifbv7NXV/mIiETl3dfg3U272fzF5jbnrZqyKsPRiIhkr7xL\nAMPnDW+zfOVFKzXsIyKSIK96xPaGflZMWkFBKC9Hu0REuixvEkB9Q32bQz9Pnf8UxYXFPkQkIpLd\n8iYBnDj/xFZl44aMY0DPAT5EIyKS/fIiASxZu6TN8lu+cUuGIxERyR15cSNYWw950+/yikg+0o1g\nCTbVb2pVVllSqc5fRKQDOX8E0Na3f93sJSL5SkcAnq07t7Yqmz5sug+RiIjknpw+AtC3fxEJGh0B\nEH3kQ0u3V9/uQyQiIrkpZxNAW498GD14tA+RiIjkppxNAC3d9PWb/A5BRCSn5GQCeOjdh1qVnXnY\nmT5EIiKSu3IyAdzwPzfs8/7oPkf7FImISO7KuQTwecPnrcoWnr3Qh0hERHJbziWAkfNH+h2CiEhe\nyLkE0NLcM+b6HYKISE7KqQTwxJonWpUd3+94HyIREcl9OZUArnvuOr9DEBHJGzmVAFp6+vyn/Q5B\nRCRn5UwC+I8X/6NVWf+e/X2IREQkP+RMAliweoHfIYiI5JWcSAARF2lV9tyFz/kQiYhI/siJBHD8\n/a2v9CnrVuZDJCIi+SMnEoCIiKRe1ieAtn405qnzn/IhEhGR/JL1CeCPtX9sVTag5wAfIhERyS9Z\nnwDuWnmX3yGIiOSlrE8ALR3Xt/XvAIuIyIHL6gTQFGlqVTZn3BwfIhERyT9ZnQDuXnV3q7JwKOxD\nJCIi+SepBGBmZWb2kJm9bWZvmtlXzay3mS01s9VmtsTMunzB/j2v35NMeCIish/JHgHcCfzVOXc0\ncDzwDjADeNo5dySwDLj2QCp8YfML/LDmhzy/6flW864adlWS4YqISIy1dZ19p1Y0Owh4zTl3eIvy\nd4BRzrk6MxsA1Djnjmpjfddy243NjQybOwyAPt378MnuT/aZv2rKKkKW1aNWIiJpZWY45ywVdSXT\nmx4GfGRmfzKzV83sHjMrBvo75+oAnHNbgH6drfDd7e/Gp1t2/oA6fxGRFEqmRy0AhgG/c84NA74g\nOvzTtUMK4P1P308iHBERORAFSay7EdjgnHvZe/8I0QRQZ2b9E4aAtrZXwaxZs+LT1dXVfHDQB+1u\n7IEzHkgiVBGR3FRTU0NNTU1a6u7yOQAAM1sOfNs5966ZzQSKvVmfOOduNrMfA72dczPaWLfVOYCr\nll3Fsg3L2txW7dTaLscpIpIvUnkOIJkjAIBpwDwzKwQ+AC4BwsBCM7sUWA9M6Gxla3esTTIcERHp\nrKQSgHNuFTC8jVmjD7SupkgT6z9fn0w4IiJyALLmsppN9ZvafPQDwAVHXJDhaERE8l/WJIC1n61t\nd961Xz2ge8lERKQTsiYBrNuxrt15BaFkT1WIiEhLWZMANP4vIpJZWZMA9ncEICIiqZc1CWDD5xva\nLO9R0CPDkYiIBENWJADnHFt3tn3D8KSjJmU4GhGRYMiKBPDpnk9pjDS2Oe+Mw87IcDQiIsGQFQmg\nvW//AEWhogxGIiISHFmfAA4qOiiDkYiIBEdWJIC6nXXtzivvUZ7BSEREgiMrEsD+jgBERCQ9lABE\nRAIqKxJA3RftDwGJiEh6ZEcC2M85ABERSY+sSAAaAhIRyTzfE8Dupt3saNgB6KmfIiKZ5HsCSPz2\n369HPx8jEREJFt8TQOL4f0VxhY+RiIgEi+8JIPEIQAlARCRzsioB9C/u72MkIiLBklUJQEcAIiKZ\n43sCSDwH0L+4PydVnhR/P7TXUD9CEhEJhKxKABXFFdww8gYOOegQKooruG3UbT5GJiKS33y/8L7l\nOYCK4gqePO9JIi6i+wJERNLI1x424iJ8tPOj+Pt+xdH7AEIWImS+H5yIiOQ1X3vZT3Z/QpNrAqCs\nWxndC7r7GY6ISKD4mgB0E5iIiH98TQBbv9AloCIifvE1AXy0O2H8X88BEhHJKH8TwE4lABERv/ia\nALbt2haf1o+/i4hkVtYkAB0BiIhklq8J4ONdH8enY/cAiIhIZmTNEUDfHn19jEREJHj8PQm8a+9J\nYCUAEZHMSjoBmFnIzF41s8e994ea2YtmttrM5ptZu4+baIpE7wIuKSyhR0GPZEMREZEDkIojgKuA\ntxLe3wzc5pw7EtgOXNZRBfr2LyKSeUklADMbBJwB/DGh+JvAI970bOC8jupRAhARybxkjwBuB/4d\ncABmVg586pyLePM3Agd3VIkuARURybwuJwAzOxOoc869BlisOGE6xnVUl24CExHJvGR+D+Ak4Bwz\nOwPoARwE3AGUmVnIOwoYBGxur4K6RdGngT7/4vPU7Kyhuro6iXBERPJPTU0NNTU1aanbnOvwC3rH\nlZiNAn7onDvHzBYA/885t8DM7gZWOef+0MY67tg/HwvADSNv4LwvdXiqQEQk8MwM51zLkZYuScd9\nADOAq83sXaAPcG9HK+gksIhI5qXkJyGdc8uB5d70h8BXD2R9PQZCRCTzsuKHd3t36+13CCIigZMV\nCaBX915+hyAiEji+J4DigmK6hbv5HYaISOD4ngB6d9fwj4iIH/xPABr/FxHxhe8JQOP/IiL+8D8B\ndFMCEBHxg+8JoKxbmd8hiIgEkv8JoEgJQETED74ngNJupX6HICISSP4ngCIlABERP/ieAHQOQETE\nH0oAIiIB5XsC0BCQiIg/fE8APQp6+B2CiEgg+Z4Awhb2OwQRkUDyPQGEzPcQREQCyffeVwlARMQf\nvve+SgAiIv7wvfc1S8mP24uIyAHyPQGE/A9BRCSQfO99NQQkIuIP33tfDQGJiPjD9wSg+wBERPzh\newLQEYCIiD98TwA6CSwi4g/fe1+dBBYR8Yfvva+GgERE/OFrAtC3fxER//ibAPw/ABERCSxfe2AN\n/4iI+MfXBKB7AERE/KMjABGRgNJJYBGRgNJJYBGRgNIQkIhIQHU5AZjZIDNbZmZvmVmtmU3zynub\n2VIzW21mS8ysrN2NawhIRMQ3yfTATcDVzrljgBOB75vZUcAM4Gnn3JHAMuDadjeuBCAi4psu98DO\nuS3Oude86XrgbWAQcC4w21tsNvCv7dXxye5Purp5ERFJUkq+gpvZocAJwItAf+dcHUSTBNAvFdsQ\nEZHUKki2AjMrAR4GrnLO1ZuZ6+y6dYvqmPXhLACqq6uprq5ONhwRkbxSU1NDTU1NWuo25zrdX7de\n2awAeBJY7Jy70yt7G6h2ztWZ2QDgv51zR7exrpvy1ynMHje75SwREWmHmeGcS8kllMkOAd0HvBXr\n/D2PAxd701OBx5LchoiIpEGXh4DM7CTgW0Ctma0EHHAdcDOw0MwuBdYDE9qrQ1cBiYj4p8sJwDn3\nPNDe09xGd6YOJQAREf/4eycwuhNYRMQvSV8FlAw9CiL7HHrooaxbt87vMCQLDB48mLVr1/odhqSR\nrwlAQ0DZZ926dSRzZZjkD31By38aAhKRdt1xxx3MmzfP7zAkTfQ0UBFpV0VFBdu2bfM7DEkT/SCM\niEhAaQhIRCSgNAQkeW/58uUccsgh8ffHHnssf//73zu1rEg+8/cqIP0kpGRI4peNN954o9PLiuQz\nHQGI5JDm5ma/Q5A8opPAkjNuvvlmJkzY99FS06dPZ/r06fz5z3/mmGOOobS0lKFDh3LPPfe0W8+Q\nIUNYtmwZALt37+biiy+mT58+HHvssbz00kv7jeGll15i5MiR9O7dm8rKSq688kqampri8998803G\njBlDeXk5AwcO5KabbgIgEonwq1/9iqFDh1JaWsrw4cPZtGkT69atIxQKEYlE4nWccsop3HfffQDM\nnj2bk08+mauvvpry8nJ+/vOf88EHH3DqqafSt29fKioqmDx5Mjt27Iivv3HjRsaPH09FRQX9+vVj\n2rRpNDQ0UF5ezptvvhlfbtu2bRQXF/Pxxx931PSSp/y9E1gngXPKcbOPS2l9tVNrD2j5iRMn8otf\n/IL6+npKSkqIRCIsXLiQRx99lI8//pi//OUvDBkyhGeffZaxY8cyYsQITjjhhP3WOWvWLD788EM+\n/PBD6uvrGTt27H6XD4fD3HHHHQwfPpwNGzYwbtw4fv/73zNt2jTq6+s57bTTuOaaa3jyySdpbGzk\nrbfeAuC2225jwYIF/O1vf2Po0KHU1tZSXFzMjh07OjwSXrFiBZMmTWLbtm00NjayceNGrrvuOkaN\nGsVnn33G+PHjmTVrFr/5zW+IRCKcddZZjB49mnnz5hEKhXj55ZcpKipi4sSJzJ07lxtvvBGA+fPn\nc9ppp1FeXn4AfwuSTzQEJDmjqqqKYcOG8eijjwLwzDPP0LNnT0aMGMG4ceMYMmQIAF//+tcZM2YM\nzz77bId1PvTQQ/z0pz+lrKyMyspKpk2btt/lhw0bxogRIzAzqqqquOKKK1i+fDkATz75JAMHDmT6\n9OkUFRXRs2dPhg8fDsC9997LL3/5S4YOHQrAcccdR+/evTu135WVlXzve98jFArRrVs3Dj/8cE49\n9VQKCgooLy/nBz/4QTyGFStW8M9//pNbbrmF7t27U1RUxMiRIwGYMmXKPjd1zZkzh4suuqhTMUh+\n8ncISCeB5QBNnDiR+fPnA9FvsJMmTQJg8eLFnHjiiZSXl9O7d28WL17MRx991GF9mzdvZtCgQfH3\ngwcPjk8/8MADHHTQQZSWlnLmmWcC8N5773H22WczcOBAevXqxU9+8pP4djZs2MDhhx/e5nY2bNjA\nYYcd1qV9bnlV0rZt25g4cSKDBg2iV69eTJ48OR7Dxo0bGTx4MKFQ68/WiBEjKCkpYfny5axevZo1\na9ZwzjnndCkmyQ++DgFpBCi3HOiQTTpMmDCBH/3oR2zatIlFixaxYsUKGhoaOP/885k7dy7nnnsu\noVCI8847r1PPNBo4cCAbNmzg6KOjP1qX+CC8SZMmxRNMzHe/+12GDRvGggULKC4u5s477+SRRx4B\noh11LDm1VFVVxZo1azjmmGP2Ke/ZsycAO3fupKSkBIAtW7bss0zLI+Vrr72WUCjEG2+8Qa9evXjs\nsce48sor4zGsX7+eSCTSZhKYOnUqc+bMYcCAAZx//vkUFRXtv4Ekr+kksOSUvn37MmrUKC655BIO\nO+wwjjjiCBoaGmhoaKBv376EQiEWL17M0qVLO1XfBRdcwI033sj27dvZuHEjv/3tb/e7/Oeff05p\naSnFxcW888473H333fF5Z511FnV1ddx11100NDRQX1/PP/7xDwAuu+wyrr/+et5//30Aamtr+fTT\nT+nbty+VlZXMnTuXSCTCfffdx5o1azqMoaSkhNLSUjZt2sSvf/3r+LwRI0YwcOBAZsyYwc6dO9mz\nZw8vvPBCfP7kyZNZtGgR8+bNY8qUKZ1qI8lfvvbAhaFCPzcvOWrSpEk888wzfOtb3wKgpKSEu+66\niwkTJtCnTx8efPBBzj333HbXT/xGPXPmTKqqqhgyZAhjx47tsFO89dZbmTdvHqWlpXznO9/hwgsv\njM8rKSnhqaee4vHHH2fAgAEcccQR8R/zvvrqq7ngggsYM2YMZWVlXH755ezatQuAe+65h1tuuYW+\nffvy9ttvc9JJJ+03hpkzZ/LKK6/Qq1cvzj77bMaPHx+fFwqFeOKJJ3jvvfeoqqrikEMOYeHChfH5\nlZWVDBs2DDPj5JNP3u92JP8l9aPwSW3YzN3wwg1cf+L1vmxf2ub94LTfYUgaXXbZZVRWVnLDDTfs\ndzkzY968eWzdupXp06dnKDrpSCp/FN7XcwBFYY0/imTS2rVrWbRoEStXrvQ7FMkCvg4BfaX/V/zc\nvEig/OxnP+PLX/4y11xzzT5XO0lw+ToEpKGG7KMhIInREFB2SuUQkC7DEREJKCUAEZGAUgIQEQko\nf+8ElqwzePBgPaNJAOjfv7/fIUiaKQHIPtauXQvAHXfcQUVFhb/BiEhaKQFIm/r168fWrVv9DkOy\nQL9+/fwOQdJEl4GKiOQQXQYqIiJJUwIQEQkoJQARkYBSAhARCSglABGRgFICEBEJKCUAEZGASksC\nMLOxZvaOmb1rZj9OxzZERCQ5KU8AZhYCfgucDvwvYKKZHZXq7eST2O/GitoikdpiL7VFeqTjCGAE\n8J5zbp1zrhF4EGj/F7pF/7gTqC32UlvspbZIj3QkgEpgQ8L7jV6ZiIhkkXQkgLaeUaGH/oiIZJmU\nPwzOzL4GzHLOjfXezwCcc+7mFsspKYiIdEGqHgaXjgQQBlYDpwL/BP4BTHTOvZ3SDYmISFJS/nsA\nzrlmM/s3YCnRIaZ71fmLiGQf334PQERE/OXLncBBuFHMzO41szozez2hrLeZLTWz1Wa2xMzKEubd\nZWbvmdlrZnZCQvlUr51Wm9mUTO9HssxskJktM7O3zKzWzKZ55UFsi25mtsLMVnptMdMrP9TMXvT2\na76ZFXjlRWb2oNcW/2NmVQl1XeuVv21mY/zap2SZWcjMXjWzx733gWwLM1trZqu8fxv/8MrS/xlx\nzmX0RTTpvA8MBgqB14CjMh1HBvbzZOAE4PWEspuBa7zpHwM3edPjgL94018FXvSmewNrgDKgV2za\n7307wHYYAJzgTZcQPT90VBDbwtuPYu/PMPCit48LgAle+d3Ad7zp7wK/96b/L/CgN30MsJLoEO6h\n3ufJ/N63LrbHD4C5wOPe+0C2BfAB0LtFWdo/I34cAQTiRjHn3HPApy2KzwVme9Oz2bvf5wL3e+ut\nAMrMrD/Ru6mXOuc+c85tJ3peZWy6Y08l59wW59xr3nQ98DYwiAC2BYBzbqc32Y1op+WAU4BHvPLZ\nwL9604lt9DDwTW/6HKIdYJNzbi3wHtHPVU4xs0HAGcAfE4q/SQDbgujl8y3747R/RvxIAEG+UazC\nOVcH0Y4RqPDK22uTluWbyOG2MrNDiR4VvQj0D2JbeEMeK4EtwFNEv6Vtd85FvEUSPw/xfXbONQOf\nmVkf8qQtgNuBf8e7T8jMyoFPA9oWDlhiZi+Z2eVeWdo/Iym/CqgTdKNYay3bxIi2Sd60lZmVEP3m\ndpVzrn4/94HkdVt4ndv/NrNSYBFwdFuLeX+2t8853xZmdiZQ55x7zcyqY8W03re8bwvPSOfcFjPr\nByw1s9W0vx8p+4z4cQSwEahKeD8I2OxDHH6o8w7VMLMBwFavfCNwSMJysTbJi7byTuQ9DMxxzj3m\nFQeyLWKcczuA5cDXgF7eQxRh3/2Kt4V3f02Zc+5T2m+jXHIScI6ZfQDMJzqkcwfR4YygtUXsGz7O\nuW3Ao0SHsdL+GfEjAbwEDDWzwWZWBFwIPO5DHJnQ8hvN48DF3vTFwGMJ5VMgfif1du/QbwlwmpmV\nmVlv4DSvLNfcB7zlnLszoSxwbWFmfWNXcphZD2A08Bbw38AEb7Gp7NsWU73pCcCyhPILvStjhgBD\nid5wmTOcc9c556qcc4cR7QOWOecmE8C2MLNi7wgZM+sJjAFqycRnxKcz3mOJXg3yHjDD7zPwadrH\nB4hm3z3AeuASomfpn/b2/SmgV8LyvyV6BcMqYFhC+cVeO70LTPF7v7rQDicBzUSv9loJvOr9/fcJ\nYFsc5+3/a8DrwE+88iHACm+/FgCFXnk3YKG3zy8ChybUda3XRm8DY/zetyTbZRR7rwIKXFt4+xz7\nfNTG+sRMfEZ0I5iISEDpJyFFRAJKCUBEJKCUAEREAkoJQEQkoJQAREQCSglARCSglABERAJKCUBE\nJKD+P55Jl9OXyUxaAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x108281690>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------\n",
      "Max Test ACC : 90.010000\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAEACAYAAAC6d6FnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1083d6110>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plot iter accuracy curve\n",
    "#Store your every iter number and accuracy in tow numpy array \"iter_log\" and \"accuracy_log\"\n",
    "#example\n",
    "train_data =  np.array([data.rstrip('\\n') for data in open('../project1/accuracy_txt/trainacc.txt')]).astype(np.float)\n",
    "valid_data =  np.array([data.rstrip('\\n') for data in open('../project1/accuracy_txt/validacc.txt')]).astype(np.float)\n",
    "test_data =  np.array([data.rstrip('\\n') for data in open('../project1/accuracy_txt/testacc.txt')]).astype(np.float)\n",
    "print \"--------------------------------------------\"\n",
    "print \"Max Train ACC : %f\"%np.max(train_data)\n",
    "iter_log = np.empty((0,5001),int)\n",
    "accuracy_log = np.empty((0,5001),float)\n",
    "for i in range(0,5001):\n",
    "    iter_log = np.append(iter_log,i)\n",
    "accuracy_log = train_data\n",
    "\n",
    "ax = plt.subplot(111)\n",
    "plt.plot(iter_log, accuracy_log, label='train-accuracy', color=\"#1f77b4\", linewidth=3)\n",
    "ax.legend(loc='upper center', bbox_to_anchor=(0.5, 0.2),  shadow=True, ncol=2)\n",
    "plt.show()\n",
    "\n",
    "print \"--------------------------------------------\"\n",
    "print \"Max Valid ACC : %f\"%np.max(valid_data)\n",
    "iter_log = np.empty((0,5001),int)\n",
    "accuracy_log = np.empty((0,5001),float)\n",
    "for i in range(0,5001):\n",
    "    iter_log = np.append(iter_log,i)\n",
    "accuracy_log = valid_data\n",
    "ax = plt.subplot(111)\n",
    "plt.plot(iter_log, accuracy_log, label='valid-accuracy', color=\"#2ca02c\", linewidth=3)\n",
    "ax.legend(loc='upper center', bbox_to_anchor=(0.5, 0.2),  shadow=True, ncol=2)\n",
    "plt.show()\n",
    "\n",
    "print \"--------------------------------------------\"\n",
    "print \"Max Test ACC : %f\"%np.max(test_data)\n",
    "iter_log = np.empty((0,5001),int)\n",
    "accuracy_log = np.empty((0,5001),float)\n",
    "for i in range(0,5001):\n",
    "    iter_log = np.append(iter_log,i)\n",
    "accuracy_log = test_data\n",
    "ax = plt.subplot(111)\n",
    "plt.plot(iter_log, accuracy_log, label='test-accuracy', color=\"#e377c2\", linewidth=3)\n",
    "ax.legend(loc='upper center', bbox_to_anchor=(0.5, 0.2),  shadow=True, ncol=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
